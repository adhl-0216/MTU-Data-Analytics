{"cells":[{"metadata":{"collapsed":true,"id":"RL8gX6o-h8Ko"},"cell_type":"markdown","source":["# Implementing linear regression with Python\n","Let's now go ahead and try to make a simple linear regression model and see what are the issues that we face and how can they be resolved to make the model more robust.\n","\n","We will use the advertising data from the lab folder.\n","\n","The following two methods implement linear regression in Python:\n","* The ols method and the statsmodel.formula.api library\n","* The scikit-learn package\n","\n","Let's implement a simple linear regression using the first method and then build upon a multiple-linear regression model.\n","\n","We will then also look at how the second method is used to do the same."]},{"metadata":{"id":"R5jReC2mh8Kr"},"cell_type":"markdown","source":["## Linear regression using the statsmodel library\n","\n","Let's first import the Advertising data, as shown:"]},{"metadata":{"trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"87yziDjth8Kr","executionInfo":{"status":"ok","timestamp":1696591883423,"user_tz":-60,"elapsed":233,"user":{"displayName":"Adrian Han Lim Oah","userId":"05324403214759422437"}},"outputId":"016c0e3f-e2e8-49c0-ad6d-c9e10c92667e"},"cell_type":"code","source":["import pandas as pd\n","\n","advert=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Week 5/Advertising.csv')\n","advert.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      TV  Radio  Newspaper  Sales\n","0  230.1   37.8       69.2   22.1\n","1   44.5   39.3       45.1   10.4\n","2   17.2   45.9       69.3    9.3\n","3  151.5   41.3       58.5   18.5\n","4  180.8   10.8       58.4   12.9"],"text/html":["\n","  <div id=\"df-9a4d7151-8901-47d8-baa6-72ad8a1ea8e4\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>TV</th>\n","      <th>Radio</th>\n","      <th>Newspaper</th>\n","      <th>Sales</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>230.1</td>\n","      <td>37.8</td>\n","      <td>69.2</td>\n","      <td>22.1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>44.5</td>\n","      <td>39.3</td>\n","      <td>45.1</td>\n","      <td>10.4</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>17.2</td>\n","      <td>45.9</td>\n","      <td>69.3</td>\n","      <td>9.3</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>151.5</td>\n","      <td>41.3</td>\n","      <td>58.5</td>\n","      <td>18.5</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>180.8</td>\n","      <td>10.8</td>\n","      <td>58.4</td>\n","      <td>12.9</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9a4d7151-8901-47d8-baa6-72ad8a1ea8e4')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-9a4d7151-8901-47d8-baa6-72ad8a1ea8e4 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-9a4d7151-8901-47d8-baa6-72ad8a1ea8e4');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-d0f201fb-6057-4eeb-975a-512252dd7898\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d0f201fb-6057-4eeb-975a-512252dd7898')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-d0f201fb-6057-4eeb-975a-512252dd7898 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":10}]},{"metadata":{"id":"m_iN3vtkh8Ks"},"cell_type":"markdown","source":["This dataset contains data about the advertising budget spent on TV, Radio, and Newspapers, for a particular product and the resulting sales. We will expect a positive correlation between such advertising costs and sales. We already know that there is a good correlation between TV advertising costs and sales.\n","\n","Let's see whether it is present or not. If yes, how does the relationship look like and to do that we write the following code:"]},{"metadata":{"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"EQoQmsCKh8Kt","executionInfo":{"status":"ok","timestamp":1696588247387,"user_tz":-60,"elapsed":1682,"user":{"displayName":"Adrian Han Lim Oah","userId":"05324403214759422437"}},"outputId":"f561e582-246a-477d-fd64-0601f22f75c5"},"cell_type":"code","source":["import statsmodels.formula.api as smf\n","model1=smf.ols(formula='Sales~TV',data=advert).fit()\n","model1.params"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Intercept    7.032594\n","TV           0.047537\n","dtype: float64"]},"metadata":{},"execution_count":9}]},{"metadata":{"id":"OOYR4w3bh8Kt"},"cell_type":"markdown","source":["In the notation that we have been using, a is the intercept and β is the slope.\n","\n","Thus:\n","$$\n","\\alpha = 7.03 and \\beta = 0.047\n","$$\n","The equation for the model will be:\n","$$\n","Sales = 7.032+0.0047*TV\n","$$"]},{"metadata":{"id":"zziou3sqh8Kt"},"cell_type":"markdown","source":["The equation implies that an increase of 100 units in advertising costs will increase the sale by four units.\n","If you remember, we learnt that the values of these parameters are estimates and there will be a p-value associated to these. If the p-values are very small, then it can be accepted that these parameters have a non-zero value and are statistically significant in the model. Let's have a look at the p-values for these parameters:"]},{"metadata":{"trusted":true,"id":"4sjJm2M7h8Ku","outputId":"32010a1d-f433-45c0-c9ec-d26e639390f9"},"cell_type":"code","source":["model1.pvalues"],"execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"Intercept    1.406300e-35\nTV           1.467390e-42\ndtype: float64"},"metadata":{}}]},{"metadata":{"id":"Gpn-uzzUh8Ku"},"cell_type":"markdown","source":["As it can be seen, the p-values are very small; hence, the parameters are significant. Let's also check another important indicator of the model efficacy and that is R2. As we saw earlier, there is a ready-made method for doing this. This can be done by typing the following code line:"]},{"metadata":{"trusted":true,"id":"6BOzcjo9h8Ku","outputId":"f1879159-28b2-4ff9-945b-3248e4088208"},"cell_type":"code","source":["model1.rsquared"],"execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"0.611875050850071"},"metadata":{}}]},{"metadata":{"id":"8Gd-kajTh8Kv"},"cell_type":"markdown","source":["The value comes out to be 0.61.\n","If we want the entire important model parameters at one go, we can take a look at the model summary by writing this snippet:"]},{"metadata":{"trusted":true,"id":"aU63clYbh8Kv","outputId":"9c901141-c13d-426a-a37b-8ce1203afda7"},"cell_type":"code","source":["model1.summary()"],"execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/html":"<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>          <td>Sales</td>      <th>  R-squared:         </th> <td>   0.612</td>\n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.610</td>\n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   312.1</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Thu, 21 Feb 2019</td> <th>  Prob (F-statistic):</th> <td>1.47e-42</td>\n</tr>\n<tr>\n  <th>Time:</th>                 <td>14:03:30</td>     <th>  Log-Likelihood:    </th> <td> -519.05</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>   200</td>      <th>  AIC:               </th> <td>   1042.</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>   198</td>      <th>  BIC:               </th> <td>   1049.</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>Intercept</th> <td>    7.0326</td> <td>    0.458</td> <td>   15.360</td> <td> 0.000</td> <td>    6.130</td> <td>    7.935</td>\n</tr>\n<tr>\n  <th>TV</th>        <td>    0.0475</td> <td>    0.003</td> <td>   17.668</td> <td> 0.000</td> <td>    0.042</td> <td>    0.053</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td> 0.531</td> <th>  Durbin-Watson:     </th> <td>   1.935</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.767</td> <th>  Jarque-Bera (JB):  </th> <td>   0.669</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td>-0.089</td> <th>  Prob(JB):          </th> <td>   0.716</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 2.779</td> <th>  Cond. No.          </th> <td>    338.</td>\n</tr>\n</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.","text/plain":"<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  Sales   R-squared:                       0.612\nModel:                            OLS   Adj. R-squared:                  0.610\nMethod:                 Least Squares   F-statistic:                     312.1\nDate:                Thu, 21 Feb 2019   Prob (F-statistic):           1.47e-42\nTime:                        14:03:30   Log-Likelihood:                -519.05\nNo. Observations:                 200   AIC:                             1042.\nDf Residuals:                     198   BIC:                             1049.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      7.0326      0.458     15.360      0.000       6.130       7.935\nTV             0.0475      0.003     17.668      0.000       0.042       0.053\n==============================================================================\nOmnibus:                        0.531   Durbin-Watson:                   1.935\nProb(Omnibus):                  0.767   Jarque-Bera (JB):                0.669\nSkew:                          -0.089   Prob(JB):                        0.716\nKurtosis:                       2.779   Cond. No.                         338.\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\"\"\""},"metadata":{}}]},{"metadata":{"id":"BFnSX2ZMh8Kv"},"cell_type":"markdown","source":["As we can see, the F-statistic for this model is very high and the associated p-value is negligible, suggesting that the parameter estimates for this model were all significant and non-zero."]},{"metadata":{"id":"4QASeaJBh8Kv"},"cell_type":"markdown","source":["Let's now predict the values of sales based on the equation we just derived. This can be done using the following snippet:"]},{"metadata":{"trusted":true,"id":"lJjfsLwDh8Kv","outputId":"2c71d508-9d2f-4f5e-b249-eea822223ef1"},"cell_type":"code","source":["sales_pred=model1.predict(pd.DataFrame(advert['TV']))\n","sales_pred"],"execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"0      17.970775\n1       9.147974\n2       7.850224\n3      14.234395\n4      15.627218\n5       7.446162\n6       9.765950\n7      12.746498\n8       7.441409\n9      16.530414\n10     10.174765\n11     17.238710\n12      8.163966\n13     11.667416\n14     16.734822\n15     16.321253\n16     10.255578\n17     20.409404\n18     10.322129\n19     14.034741\n20     17.414596\n21     18.317792\n22      7.660077\n23     17.885209\n24      9.994126\n25     19.529976\n26     13.825579\n27     18.446141\n28     18.859710\n29     10.388680\n         ...    \n170     9.409426\n171    14.852371\n172     7.964312\n173    15.037764\n174    17.604742\n175    20.195489\n176    18.840695\n177    15.123330\n178    20.185982\n179    14.904661\n180    14.476831\n181    17.419349\n182     9.704153\n183    20.704131\n184    19.097393\n185    16.777605\n186    13.663955\n187    16.116846\n188    20.628073\n189     7.921529\n190     8.910291\n191    10.621610\n192     7.850224\n193    14.961705\n194    14.148829\n195     8.848493\n196    11.510545\n197    15.446579\n198    20.513985\n199    18.065848\nLength: 200, dtype: float64"},"metadata":{}}]},{"metadata":{"id":"3kPnM0Lsh8Kv"},"cell_type":"markdown","source":["This equation basically calculates the predicted sales value for each row based on the model equation using TV costs. One can plot sales_pred against the TV advertising costs to find the line of best fit. Let's do that:"]},{"metadata":{"trusted":true,"id":"HrkV2Uuyh8Kw","outputId":"394583d4-0dda-4a1e-d358-5f7de346c751"},"cell_type":"code","source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","advert.plot(kind='scatter', x='TV', y='Sales')\n","plt.plot(pd.DataFrame(advert['TV']),sales_pred,c='red',linewidth=2)"],"execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"[<matplotlib.lines.Line2D at 0x7fe38db130f0>]"},"metadata":{}},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"}}]},{"metadata":{"id":"fAIIsVEBh8Kw"},"cell_type":"markdown","source":["We get the following plot as the output. The red line is the line of best fit (obtained from the model). The blue dots represent the actual data present:"]},{"metadata":{"id":"iwdwJa9bh8Kw"},"cell_type":"markdown","source":["Now, let's calculate the RSE term for our prediction using the following code snippet:"]},{"metadata":{"trusted":true,"id":"6aw-dDEfh8Kw","outputId":"0e0a97b4-b218-45ee-8ed4-e8f6cdb51fa4"},"cell_type":"code","source":["import numpy as np\n","advert['sales_pred']=0.047537*advert['TV']+7.03\n","advert['RSE']=(advert['Sales']-advert['sales_pred'])**2\n","RSEd=advert.sum()['RSE']\n","RSE=np.sqrt(RSEd/198)\n","salesmean=np.mean(advert['Sales'])\n","error=RSE/salesmean\n","RSE,salesmean,error"],"execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"(3.258657369247128, 14.022500000000003, 0.23238776033140504)"},"metadata":{}}]},{"metadata":{"id":"2jRr5nn1h8Kw"},"cell_type":"markdown","source":["The output consists of three numbers, first of which is RSE=3.25, second is salesmean (mean of actual sales) = 14.02 and error is their ratio, which is equal to 0.23. Thus, on an average this model will have 23%, even if the coefficients are correctly predicted. This is a significant amount of errors and we would like to bring it down by some means. Also, the R2 value of 0.61 can be improved upon. One thing we can try is to add more columns in the model, as predictors and see whether it improves the result or not."]},{"metadata":{"id":"PkQwM_lDh8Kw"},"cell_type":"markdown","source":["## Multiple linear regression\n","When linear regression involves more than one predictor variable, then it is called multiple linear regression. The nature of the model remains the same (linear), except that there might be separate slope (β) coefficients associated with each of the predictor variables. The model will be represented, as follows:\n","$$\n","y model = \\alpha  + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 ......+\\beta_n X_n\n","$$\n","\n","Each βi will be estimated using the same least sum of squares method; hence, would have a p-value associated with the estimation. The smaller the p-value associated with a variable, the more the significance of that variable to the model. The variables with large p-values should be eliminated from the model as they aren't good predictors of the output variable.\n","\n","While the multiple regression gives us with the possibility of using more variables as predictors; hence, it increases the efficiency of the model. It also increases the complexity of the process of model building, as the selection of the variables to be kept and discarded in the model becomes tedious.\n","\n","With this simple dataset of three predictor variables, there can be seven possible models.\n","\n","They are as follows:\n","* Model 1: Sales~TV\n","* Model 2: Sales~Newspaper\n","* Model 3: Sales~Radio\n","* Model 4: Sales~TV+Radio\n","* Model 5: Sales~TV+Newspaper\n","* Model 6: Sales~Newspaper+Radio\n","* Model 7: Sales~TV+Radio+Newspaper\n","\n","For a model with p possible predictor variables, there can be 2p-1 possible models; hence, as the number of predictors increases, the selection becomes tedious.\n","\n","It would have been a tedious task to choose from so many possible models. Thankfully, there are a few guidelines to filter some of these and then navigate towards the most efficient one.\n","The following are the guidelines:\n","* Keep the variables with low p-values and eliminate the ones with high p-values\n","* Inclusion of a variable to the model should ideally increase the value of R2 (although it is not a very reliable indicator of the same and looking at the adjusted R2 is preferred. The concept of adjusted R2 and why it is a better indicator than R2 will be explained later).\n","\n","Based on these guidelines, there are two kinds of approaches to select the predictor variables to go in the final model:\n","\n","* Forward selection: In this approach, we start with a null model without any predictor and then start adding predictor variables one by one. The variable whose addition results into a model with the lowest residual sum of squares will be added first to the model. If the p-value for the variable is small enough and the value of the adjusted R2 goes up; the predictor variable is included in the model. Otherwise, it is not included in the model.\n","* Backward selection: In this approach, one starts with a model that has all the possible predictor variables in the model and discards some of them. If the p-value of a predictor variable is large and the value of the adjusted R2 goes up, the predictor variable is discarded from the model. Otherwise, it remains a part of the model.\n","\n","Many of the statistical programs, including the Python, give us an option to select from the two preceding approaches while implementing a linear regression. The statistical program then implements the linear regression using the selected approach.\n","\n","For now, let us manually add a few variables and see how it changes the model parameters and efficacy, so that we can get a better glimpse of what goes on behind the curtain when these approaches are implemented by the statistical program."]},{"metadata":{"id":"YCCbU5KMh8Kw"},"cell_type":"markdown","source":["We have already seen one model assuming a linear relationship between sales and TV advertising costs. We can ignore the other models consisting of single variables (that is newspaper and radio, as they have a small correlation compared to TV). Let us now try to add more variables to the model we already have and see how the parameters and efficacy change.\n","\n","Let us try adding the newspaper variable to the model using the following code snippet:"]},{"metadata":{"trusted":true,"id":"LPocJP97h8Kx","outputId":"a7fd583a-e582-44ed-c0a7-49ef2a4fa6dd"},"cell_type":"code","source":["import statsmodels.formula.api as smf\n","model2=smf.ols(formula='Sales~TV+Newspaper',data=advert).fit()\n","model2.params"],"execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"Intercept    5.774948\nTV           0.046901\nNewspaper    0.044219\ndtype: float64"},"metadata":{}}]},{"metadata":{"id":"UJsJyQz0h8Kx"},"cell_type":"markdown","source":["The p-values for the coefficients are very small, suggesting that all the estimates are significant. The equation for this model will be:\n","\n","$$\n","Sales = 5.77 +0.046*TV+0.04*Newspaper\n","$$\n","\n","The values of $R^2$ and adjusted $R^2$ are 0.646 and 0.642, which is just a minor improvement from the value obtained in the earlier model.\n","The values can be predicted using the following snippet:"]},{"metadata":{"trusted":true,"id":"j9xhSeNch8Kx","outputId":"0a059292-6996-41c2-bf8a-05f2cfb27f12"},"cell_type":"code","source":["sales_pred=model2.predict(advert[['TV','Newspaper']])\n","sales_pred"],"execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"0      19.626901\n1       9.856348\n2       9.646055\n3      15.467318\n4      16.837102\n5       9.499445\n6       9.510924\n7      11.925419\n8       6.222518\n9      16.083262\n10      9.945228\n11     16.021516\n12      9.805257\n13     10.666196\n14     17.381579\n15     17.278653\n16     13.995865\n17     21.440393\n18      9.829727\n19     13.528088\n20     18.379490\n21     17.948453\n22      8.587327\n23     17.641044\n24      9.506109\n25     18.967556\n26     13.034296\n27     18.048554\n28     18.456595\n29     10.890326\n         ...    \n170     8.933646\n171    15.586198\n172     7.445942\n173    14.239121\n174    16.785052\n175    20.610266\n176    18.322864\n177    15.314058\n178    19.800514\n179    14.320051\n180    13.486699\n181    17.234475\n182     9.724113\n183    22.438692\n184    19.005059\n185    16.256398\n186    13.493904\n187    15.542563\n188    19.352307\n189     7.686735\n190     7.884019\n191     9.581306\n192     7.978983\n193    13.757260\n194    13.061376\n195     8.176802\n196    10.551220\n197    14.359467\n198    22.003458\n199    17.045429\nLength: 200, dtype: float64"},"metadata":{}}]},{"metadata":{"id":"89ANnZC_h8Kx"},"cell_type":"markdown","source":["To calculate the RSE, we modify the snippet a little"]},{"metadata":{"trusted":true,"id":"2jQ5EZaNh8Kx","outputId":"cf04c8fe-6504-41fd-a1bb-fd08912fd1eb"},"cell_type":"code","source":["import numpy as np\n","advert['sales_pred']=5.77 + 0.046*advert['TV'] + 0.04*advert['Newspaper']\n","advert['RSE']=(advert['Sales']-advert['sales_pred'])**2\n","RSEd=advert.sum()['RSE']\n","RSE=np.sqrt(RSEd/197)\n","salesmean=np.mean(advert['Sales'])\n","error=RSE/salesmean\n","RSE,salesmean,error"],"execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"(3.1346969895743846, 14.022500000000003, 0.22354765481008265)"},"metadata":{}}]},{"metadata":{"id":"dvaNZbKWh8Kx"},"cell_type":"markdown","source":["The value of RSE comes out to be 3.12 (22%), not very different from the model with only TV. The number 197 comes from the (n-p-1) term in the formula for RSE, where n=200, p=2 for the current model. The following table is the model summary:"]},{"metadata":{"trusted":true,"id":"o4Tz2b9Vh8Kx","outputId":"7eb804b8-bb68-4b67-c467-0e329a19ded0"},"cell_type":"code","source":["model2.summary()"],"execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/html":"<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>          <td>Sales</td>      <th>  R-squared:         </th> <td>   0.646</td>\n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.642</td>\n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   179.6</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Thu, 21 Feb 2019</td> <th>  Prob (F-statistic):</th> <td>3.95e-45</td>\n</tr>\n<tr>\n  <th>Time:</th>                 <td>14:05:21</td>     <th>  Log-Likelihood:    </th> <td> -509.89</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>   200</td>      <th>  AIC:               </th> <td>   1026.</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>   197</td>      <th>  BIC:               </th> <td>   1036.</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>Intercept</th> <td>    5.7749</td> <td>    0.525</td> <td>   10.993</td> <td> 0.000</td> <td>    4.739</td> <td>    6.811</td>\n</tr>\n<tr>\n  <th>TV</th>        <td>    0.0469</td> <td>    0.003</td> <td>   18.173</td> <td> 0.000</td> <td>    0.042</td> <td>    0.052</td>\n</tr>\n<tr>\n  <th>Newspaper</th> <td>    0.0442</td> <td>    0.010</td> <td>    4.346</td> <td> 0.000</td> <td>    0.024</td> <td>    0.064</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td> 0.658</td> <th>  Durbin-Watson:     </th> <td>   1.969</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.720</td> <th>  Jarque-Bera (JB):  </th> <td>   0.415</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td>-0.093</td> <th>  Prob(JB):          </th> <td>   0.813</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 3.122</td> <th>  Cond. No.          </th> <td>    410.</td>\n</tr>\n</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.","text/plain":"<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  Sales   R-squared:                       0.646\nModel:                            OLS   Adj. R-squared:                  0.642\nMethod:                 Least Squares   F-statistic:                     179.6\nDate:                Thu, 21 Feb 2019   Prob (F-statistic):           3.95e-45\nTime:                        14:05:21   Log-Likelihood:                -509.89\nNo. Observations:                 200   AIC:                             1026.\nDf Residuals:                     197   BIC:                             1036.\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      5.7749      0.525     10.993      0.000       4.739       6.811\nTV             0.0469      0.003     18.173      0.000       0.042       0.052\nNewspaper      0.0442      0.010      4.346      0.000       0.024       0.064\n==============================================================================\nOmnibus:                        0.658   Durbin-Watson:                   1.969\nProb(Omnibus):                  0.720   Jarque-Bera (JB):                0.415\nSkew:                          -0.093   Prob(JB):                        0.813\nKurtosis:                       3.122   Cond. No.                         410.\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\"\"\""},"metadata":{}}]},{"metadata":{"id":"bCfi0p6wh8Ky"},"cell_type":"markdown","source":["Although as the F-statistic decreases, the associated p-value also decreases. But, it is just a marginal improvement to the model, as we can see in the adj. R2 value. So, adding newspaper didn't improve the model significantly.\n","\n","Let's try adding radio to the model instead of the newspaper. Radio had the second best correlation with the Sales variable in the correlation matrix we created earlier in the chapter. Thus, one expects some significant improvement in the model upon its addition to the model. Let's see if that happens or not:"]},{"metadata":{"trusted":true,"id":"dbOKpWu_h8Ky","outputId":"d7441683-7f95-4397-ef98-5bc54d096651"},"cell_type":"code","source":["import statsmodels.formula.api as smf\n","model3=smf.ols(formula='Sales~TV+Radio',data=advert).fit()\n","model3.params"],"execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"Intercept    2.921100\nTV           0.045755\nRadio        0.187994\ndtype: float64"},"metadata":{}}]},{"metadata":{"id":"UmXlt0Bwh8Ky"},"cell_type":"markdown","source":["The model can be represented as the following:\n","$$\n","Sales = 2.92 + 0.045 * TV + 0.18 * Radio\n","$$\n","The values can be predicted based on the preceding model using the following snippet:"]},{"metadata":{"trusted":true,"id":"wxjWOxXeh8Ky","outputId":"706e7657-95e0-4a9a-d216-94e8f8ce857a"},"cell_type":"code","source":["sales_pred=model3.predict(advert[['TV','Radio']])\n","sales_pred"],"execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"0      20.555465\n1      12.345362\n2      12.337018\n3      17.617116\n4      13.223908\n5      12.512084\n6      11.718212\n7      12.105516\n8       3.709379\n9      12.551697\n10      7.035860\n11     17.256520\n12     10.608662\n13      8.810951\n14     18.444668\n15     20.828915\n16     12.903865\n17     23.241076\n18      9.941215\n19     14.153846\n20     18.121392\n21     14.742064\n22      6.514172\n23     16.544027\n24      8.140352\n25     15.608021\n26     14.967694\n27     17.046335\n28     19.399541\n29      9.159297\n         ...    \n170     7.389574\n171    14.376846\n172     7.596578\n173    11.960970\n174    13.736151\n175    24.783526\n176    19.964022\n177    12.174924\n178    16.013844\n179    12.378040\n180    10.575089\n181    13.933696\n182     6.564088\n183    24.163936\n184    18.537949\n185    20.779377\n186     9.698684\n187    17.060279\n188    18.620097\n189     6.051445\n190    12.454978\n191     8.405926\n192     4.478859\n193    18.448761\n194    16.463190\n195     5.364512\n196     8.152375\n197    12.768048\n198    23.792923\n199    15.157543\nLength: 200, dtype: float64"},"metadata":{}}]},{"metadata":{"id":"U9jJf_-Jh8Ky"},"cell_type":"markdown","source":["The model summary looks like:"]},{"metadata":{"trusted":true,"scrolled":true,"id":"LiW8Rwg8h8Ky","outputId":"d792cf8f-fd6b-4e34-84f1-b1919c9f198d"},"cell_type":"code","source":["model3.summary()"],"execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":20,"data":{"text/html":"<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>          <td>Sales</td>      <th>  R-squared:         </th> <td>   0.897</td>\n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.896</td>\n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   859.6</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Thu, 21 Feb 2019</td> <th>  Prob (F-statistic):</th> <td>4.83e-98</td>\n</tr>\n<tr>\n  <th>Time:</th>                 <td>14:08:45</td>     <th>  Log-Likelihood:    </th> <td> -386.20</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>   200</td>      <th>  AIC:               </th> <td>   778.4</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>   197</td>      <th>  BIC:               </th> <td>   788.3</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>Intercept</th> <td>    2.9211</td> <td>    0.294</td> <td>    9.919</td> <td> 0.000</td> <td>    2.340</td> <td>    3.502</td>\n</tr>\n<tr>\n  <th>TV</th>        <td>    0.0458</td> <td>    0.001</td> <td>   32.909</td> <td> 0.000</td> <td>    0.043</td> <td>    0.048</td>\n</tr>\n<tr>\n  <th>Radio</th>     <td>    0.1880</td> <td>    0.008</td> <td>   23.382</td> <td> 0.000</td> <td>    0.172</td> <td>    0.204</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>60.022</td> <th>  Durbin-Watson:     </th> <td>   2.081</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 148.679</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td>-1.323</td> <th>  Prob(JB):          </th> <td>5.19e-33</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 6.292</td> <th>  Cond. No.          </th> <td>    425.</td>\n</tr>\n</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.","text/plain":"<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  Sales   R-squared:                       0.897\nModel:                            OLS   Adj. R-squared:                  0.896\nMethod:                 Least Squares   F-statistic:                     859.6\nDate:                Thu, 21 Feb 2019   Prob (F-statistic):           4.83e-98\nTime:                        14:08:45   Log-Likelihood:                -386.20\nNo. Observations:                 200   AIC:                             778.4\nDf Residuals:                     197   BIC:                             788.3\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      2.9211      0.294      9.919      0.000       2.340       3.502\nTV             0.0458      0.001     32.909      0.000       0.043       0.048\nRadio          0.1880      0.008     23.382      0.000       0.172       0.204\n==============================================================================\nOmnibus:                       60.022   Durbin-Watson:                   2.081\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              148.679\nSkew:                          -1.323   Prob(JB):                     5.19e-33\nKurtosis:                       6.292   Cond. No.                         425.\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\"\"\""},"metadata":{}}]},{"metadata":{"trusted":true,"id":"lOLKql9Oh8Ky"},"cell_type":"markdown","source":["One thing to observe here is that the R2 value has improved considerably due to the addition of radio to the model. Also, the F-statistic has increased significantly from the last model indicating a very efficient model.\n","\n","The RSE can be calculated using the same method described previously. The value for this model comes out to be 1.71 (around 12%),which is much better than the 23% and 22% in the previous model.\n","\n","Thus, we can conclude that radio is a great addition to the model and TV and radio advertising costs have been able to describe the sales very well and this model itself is a very efficient model. But, can we improve it a bit further by combining all three predictor variables?\n","\n","The last thing that we should try is, all the predictor variables together by using the following code: The estimates of the coefficients and the associated p-values for this model are shown:"]},{"metadata":{"trusted":true,"id":"AwZP3kGPh8Ky","outputId":"53c514e5-2b9f-41b7-e9b5-b89932ab7b65"},"cell_type":"code","source":["import statsmodels.formula.api as smf\n","model4=smf.ols(formula='Sales~TV+Radio+Newspaper',data=advert).fit()\n","model4.params"],"execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"Intercept    2.938889\nTV           0.045765\nRadio        0.188530\nNewspaper   -0.001037\ndtype: float64"},"metadata":{}}]},{"metadata":{"id":"R5RL8aalh8Kz"},"cell_type":"markdown","source":["The p-values for the coefficients are very small, suggesting that all the estimates are significant. The equation for this model will be:\n","$$\n","Sales = 2.93 + 0.045 * TV + 0.18 * Radio - 0.01 * Newpaper\n","$$\n","\n","The values of sales can be predicted using the following snippet:"]},{"metadata":{"trusted":true,"id":"wLp2wwoph8K2","outputId":"1a9abf34-a8a8-4660-c147-2116e19ec072"},"cell_type":"code","source":["sales_pred=model4.predict(advert[['TV','Radio','Newspaper']])\n","sales_pred"],"execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"0      20.523974\n1      12.337855\n2      12.307671\n3      17.597830\n4      13.188672\n5      12.478348\n6      11.729760\n7      12.122953\n8       3.727341\n9      12.550849\n10      7.032299\n11     17.285129\n12     10.577121\n13      8.826300\n14     18.434366\n15     20.819300\n16     12.823657\n17     23.224957\n18      9.951682\n19     14.166073\n20     18.100767\n21     14.740538\n22      6.489150\n23     16.545933\n24      8.146519\n25     15.610039\n26     14.989514\n27     17.051673\n28     19.410538\n29      9.144024\n         ...    \n170     7.394980\n171    14.358274\n172     7.607692\n173    11.970939\n174    13.744357\n175    24.786870\n176    19.979373\n177    12.162046\n178    16.010997\n179    12.384555\n180    10.587200\n181    13.928099\n182     6.554670\n183    24.133100\n184    18.538521\n185    20.803011\n186     9.691373\n187    17.076442\n188    18.644306\n189     6.051624\n190    12.489159\n191     8.424019\n192     4.466230\n193    18.486958\n194    16.495300\n195     5.370342\n196     8.165312\n197    12.785921\n198    23.767321\n199    15.173196\nLength: 200, dtype: float64"},"metadata":{}}]},{"metadata":{"id":"xyYuwjDQh8K2"},"cell_type":"markdown","source":["The summary of the model is shown in the following table:"]},{"metadata":{"trusted":true,"id":"7HZL97IVh8K2","outputId":"aafb6683-9393-48cc-df98-717cb0052a56"},"cell_type":"code","source":["model4.summary()"],"execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":21,"data":{"text/html":"<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>          <td>Sales</td>      <th>  R-squared:         </th> <td>   0.897</td>\n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.896</td>\n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   570.3</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Thu, 21 Feb 2019</td> <th>  Prob (F-statistic):</th> <td>1.58e-96</td>\n</tr>\n<tr>\n  <th>Time:</th>                 <td>14:08:55</td>     <th>  Log-Likelihood:    </th> <td> -386.18</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>   200</td>      <th>  AIC:               </th> <td>   780.4</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>   196</td>      <th>  BIC:               </th> <td>   793.6</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>Intercept</th> <td>    2.9389</td> <td>    0.312</td> <td>    9.422</td> <td> 0.000</td> <td>    2.324</td> <td>    3.554</td>\n</tr>\n<tr>\n  <th>TV</th>        <td>    0.0458</td> <td>    0.001</td> <td>   32.809</td> <td> 0.000</td> <td>    0.043</td> <td>    0.049</td>\n</tr>\n<tr>\n  <th>Radio</th>     <td>    0.1885</td> <td>    0.009</td> <td>   21.893</td> <td> 0.000</td> <td>    0.172</td> <td>    0.206</td>\n</tr>\n<tr>\n  <th>Newspaper</th> <td>   -0.0010</td> <td>    0.006</td> <td>   -0.177</td> <td> 0.860</td> <td>   -0.013</td> <td>    0.011</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>60.414</td> <th>  Durbin-Watson:     </th> <td>   2.084</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 151.241</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td>-1.327</td> <th>  Prob(JB):          </th> <td>1.44e-33</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 6.332</td> <th>  Cond. No.          </th> <td>    454.</td>\n</tr>\n</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.","text/plain":"<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  Sales   R-squared:                       0.897\nModel:                            OLS   Adj. R-squared:                  0.896\nMethod:                 Least Squares   F-statistic:                     570.3\nDate:                Thu, 21 Feb 2019   Prob (F-statistic):           1.58e-96\nTime:                        14:08:55   Log-Likelihood:                -386.18\nNo. Observations:                 200   AIC:                             780.4\nDf Residuals:                     196   BIC:                             793.6\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      2.9389      0.312      9.422      0.000       2.324       3.554\nTV             0.0458      0.001     32.809      0.000       0.043       0.049\nRadio          0.1885      0.009     21.893      0.000       0.172       0.206\nNewspaper     -0.0010      0.006     -0.177      0.860      -0.013       0.011\n==============================================================================\nOmnibus:                       60.414   Durbin-Watson:                   2.084\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              151.241\nSkew:                          -1.327   Prob(JB):                     1.44e-33\nKurtosis:                       6.332   Cond. No.                         454.\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\"\"\""},"metadata":{}}]},{"metadata":{"id":"KpSpu2rOh8K2"},"cell_type":"markdown","source":["The most striking feature of this model is that the estimate of the coefficients is very similar to that in the previous model. The intercept, coefficient for TV, and the coefficient for Radio are more or less the same. The values of R2 and adj-R2 are also similar to the previous model.\n","\n","The value of RSE can be calculated in a similar way, as before. The value comes out to 2.57 (18%), which is more than the previous model.\n","\n","Other things to note about this model are the following:\n","\n","* There is a small negative coefficient for the newspaper. When we considered only TV and newspaper in the model, the coefficient of the newspaper was significantly positive. Something affected the coefficient of the newspaper when it became a part of the model in presence of TV and radio.\n","* For this model, the F-statistic has decreased considerably to 570.3 from 859.6 in the previous model. This suggests that the partial benefit of adding newspaper to the model containing TV and radio is negative.\n","* The value of RSE increases on addition of newspaper to the model.\n","\n","All these point in the direction that the model actually became a little less efficient on addition of newspaper to the previous model. What is the reason? Multi-collinearity is the reason for the sub-optimal performance of the model when newspaper was added to the final model. Multi-collinearity alludes to the correlation between the predictor variables of the model. Go back to the correlation matrix that we created for this dataset and you will find that there is a significant correlation of 0.35 between radio and newspaper. This means that the expense on Newspaper is related to that on the Radio. This relationship between the predictor variable increases the variability of the co-efficient estimates of the related predictor variables"]},{"metadata":{"id":"xrPZYRcIh8K3"},"cell_type":"markdown","source":["# Model validation\n","Any predictive model needs to be validated to see how it is performing on different sets of data, whether the accuracy of the model is constant over all the sources of similar data or not. This checks the problem of over-fitting, wherein the model fits very well on one set of data but doesn't fit that well on another dataset. One common method is to validate a model traintest split of the dataset. Another method is k-fold cross validation, about which we will learn more in the later chapter.\n","\n","## Training and testing data split\n","Ideally, this step should be done right at the onset of the modelling process so that there are no sampling biases in the model; in other words, the model should perform well even for a dataset that has the same predictor variables, but their means and variances are very different from what the model has been built upon. This can happen because the dataset on which the model is built (training) and the one on which it is applied (testing) can come from different sources.\n","\n","A more robust way to do this is a process called the k-fold cross validation.\n","\n","Let's see how we can split the available dataset in the training and testing dataset and apply the model to the testing dataset to get other results:"]},{"metadata":{"trusted":true,"id":"s7qUQy5uh8K3"},"cell_type":"code","source":["import numpy as np\n","a=np.random.randn(len(advert))\n","check=a<0.8\n","training=advert[check]\n","testing=advert[~check]"],"execution_count":null,"outputs":[]},{"metadata":{"id":"ryyRSSYth8K3"},"cell_type":"markdown","source":["NOTE:\n","The above sampling technique is naïve and doesn’t consider sampling etc. You might remember from previous labs that Scikitlearn has a very helpful function train_test_split which automates the process of selecting a training and testing set.\n","\n","http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html   \n","\n","The ratio of split between training and testing datasets is 80:20; in other words, 160 rows of the advert dataset will be in training and 40 rows in testing.\n","Let's create a model on training the data and test the model performance on the testing data. Let us create the only model that works best (we have found it already), the one with TV and radio variables, as predictor variables:"]},{"metadata":{"trusted":true,"id":"w5kmgMcFh8K3","outputId":"f4333eef-513d-43f8-c697-14a1bc1199b6"},"cell_type":"code","source":["import statsmodels.formula.api as smf\n","model5=smf.ols(formula='Sales~TV+Radio',data=training).fit()\n","model5.summary()"],"execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":24,"data":{"text/html":"<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>          <td>Sales</td>      <th>  R-squared:         </th> <td>   0.901</td>\n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.900</td>\n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   684.9</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Thu, 21 Feb 2019</td> <th>  Prob (F-statistic):</th> <td>3.75e-76</td>\n</tr>\n<tr>\n  <th>Time:</th>                 <td>14:12:45</td>     <th>  Log-Likelihood:    </th> <td> -295.00</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>   153</td>      <th>  AIC:               </th> <td>   596.0</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>   150</td>      <th>  BIC:               </th> <td>   605.1</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>Intercept</th> <td>    2.9233</td> <td>    0.327</td> <td>    8.928</td> <td> 0.000</td> <td>    2.276</td> <td>    3.570</td>\n</tr>\n<tr>\n  <th>TV</th>        <td>    0.0444</td> <td>    0.002</td> <td>   28.391</td> <td> 0.000</td> <td>    0.041</td> <td>    0.048</td>\n</tr>\n<tr>\n  <th>Radio</th>     <td>    0.1977</td> <td>    0.009</td> <td>   20.919</td> <td> 0.000</td> <td>    0.179</td> <td>    0.216</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>60.936</td> <th>  Durbin-Watson:     </th> <td>   2.179</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 216.604</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td>-1.496</td> <th>  Prob(JB):          </th> <td>9.23e-48</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 8.003</td> <th>  Cond. No.          </th> <td>    416.</td>\n</tr>\n</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.","text/plain":"<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  Sales   R-squared:                       0.901\nModel:                            OLS   Adj. R-squared:                  0.900\nMethod:                 Least Squares   F-statistic:                     684.9\nDate:                Thu, 21 Feb 2019   Prob (F-statistic):           3.75e-76\nTime:                        14:12:45   Log-Likelihood:                -295.00\nNo. Observations:                 153   AIC:                             596.0\nDf Residuals:                     150   BIC:                             605.1\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      2.9233      0.327      8.928      0.000       2.276       3.570\nTV             0.0444      0.002     28.391      0.000       0.041       0.048\nRadio          0.1977      0.009     20.919      0.000       0.179       0.216\n==============================================================================\nOmnibus:                       60.936   Durbin-Watson:                   2.179\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              216.604\nSkew:                          -1.496   Prob(JB):                     9.23e-48\nKurtosis:                       8.003   Cond. No.                         416.\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\"\"\""},"metadata":{}}]},{"metadata":{"id":"8Dou1zKOh8K3"},"cell_type":"markdown","source":["Most of the model parameters, such as intercept, coefficient estimates, and R2 are very similar. The difference in F-statistics can be attributed to a smaller dataset. The smaller the dataset, the larger the value of SSD and the smaller the value of the (n-p-1) term in F-statistic formula; both contribute towards the decrease in the F-statistic value.\n","\n","The model can be written, as follows:\n","$$\n","Sales ~ 2.86 + 0.04 * TV + 0.17 * Radio\n","$$\n","Let us now predict the sales values for the testing dataset:"]},{"metadata":{"trusted":true,"id":"N2Oh4etvh8K3","outputId":"49e50adb-d42d-424a-b1a1-7ec84f99fd76"},"cell_type":"code","source":["sales_pred=model5.predict(training[['TV','Radio']])\n","sales_pred"],"execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":25,"data":{"text/plain":"0      20.621982\n3      17.820886\n4      13.093285\n6      11.963060\n7      12.139905\n8       3.720667\n9      12.316553\n10      7.007473\n11     17.209400\n12     10.920112\n13      8.758759\n14     18.497819\n18     10.051361\n20     18.105303\n22      6.653290\n23     16.410154\n24      8.182928\n25     15.298680\n27     16.895014\n29      9.223949\n30     21.534744\n31     11.380559\n32      7.539485\n33     18.680644\n34      7.453054\n35     16.652745\n36     23.443567\n37     16.009182\n38     10.117174\n39     20.508887\n         ...    \n164    11.037876\n165    14.016798\n166    11.152151\n167    13.141645\n168    17.161430\n169    17.653346\n170     7.438613\n172     7.768030\n174    13.479067\n175    24.896220\n176    19.932759\n177    12.029128\n178    15.674725\n180    10.396720\n181    13.701139\n182     6.547742\n183    24.205331\n184    18.413249\n185    20.949700\n189     6.146471\n190    12.804002\n191     8.413692\n192     4.498246\n193    18.639214\n194    16.614029\n195     5.352420\n196     8.078327\n197    12.627867\n198    23.829873\n199    14.938157\nLength: 153, dtype: float64"},"metadata":{}}]},{"metadata":{"id":"SDepTuKYh8K3"},"cell_type":"markdown","source":["The value of RSE for this prediction on the testing dataset can be calculated using the following snippet:"]},{"metadata":{"trusted":true,"id":"w6WWch8Fh8K3","outputId":"c405ee72-8f97-459e-9917-0b396c78cb9b"},"cell_type":"code","source":["import numpy as np\n","testing['sales_pred']=2.86 + 0.04*testing['TV'] + 0.17*testing['Radio']\n","testing['RSE']=(testing['Sales']-testing['sales_pred'])**2\n","RSEd=testing.sum()['RSE']\n","RSE=np.sqrt(RSEd/51)\n","salesmean=np.mean(testing['Sales'])\n","error=RSE/salesmean\n","RSE,salesmean,error"],"execution_count":null,"outputs":[{"output_type":"stream","text":"/home/nbuser/anaconda3_501/lib/python3.6/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  from ipykernel import kernelapp as app\n/home/nbuser/anaconda3_501/lib/python3.6/site-packages/ipykernel/__main__.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  app.launch_new_instance()\n","name":"stderr"},{"output_type":"execute_result","execution_count":27,"data":{"text/plain":"(2.098512096607461, 14.404255319148932, 0.14568695500819898)"},"metadata":{}}]},{"metadata":{"id":"JT-4sxHCh8K4"},"cell_type":"markdown","source":["The value of RSE comes out to be 2.54 over a sales mean (in the testing data) of 14.80 amounting to an error of 17%.\n","\n","We can see that the model doesn't generalize very well on the testing dataset, as the RSE for the same model is different in the two cases. It implies some degree of over fitting when we tried to build the model based on the entire dataset. The RSE with the training-testing split, albeit a bit more, is more reliable and replicable.\n","\n","## Summary of models\n","We have tried four models previously. Let us summarize the major results from each of the models, at one place:\n","\n","![image.png](attachment:image.png)\n","\n","To summarize, for a good linear model, the predictor variables should be chosen based on the following criteria:\n","* $R^2$: $R^2$ will always increase when you add a new predictor variable to the model. However, it is not a very reliable check of the increased efficiency of the model. Rather, for an efficient model, we should check the **adjusted-$R^2$**. This should increase on adding a new predictor variable.\n","* **p-values:** The lower the p-value for the estimate of the predictor variable, the better it is to add the predictor variable to the model.\n","* **F-statistic:** The value of the F-statistic for the model should increase after the addition of a new predictor variable for a predictor variable to be an efficient addition to the model. The increase in the F-statistic is a proxy to the improvement in the model brought upon solely by the addition of that particular variable. Alternatively, the p-value associated with the F-statistic should decrease on the addition of a new predictor variable. • RSE: The value of RSE for the new model should decrease on the addition of the new predictor variable.\n","* **VIF:** To take care of the issues arising due to multi-collinearity one needs to eliminate the variables with large values of VIF.\n","\n"]},{"metadata":{"id":"hnxOYjb4h8K4"},"cell_type":"markdown","source":["# Linear regression with scikit-learn\n","Let's now re-implement the linear regression model using the scikit-learn package. This method is more elegant as it has more in-built methods to perform the regular processes associated with regression.\n","\n","For example, you might remember from the last lab that there is a separate method for splitting the dataset into training and testing datasets:"]},{"metadata":{"trusted":true,"id":"WSHp3CF4h8K4","outputId":"b25236fe-3f39-45ae-bef3-5aa14d0de23b"},"cell_type":"code","source":["from sklearn.linear_model import LinearRegression\n","from sklearn.cross_validation import train_test_split\n","\n","feature_cols = ['TV', 'Radio']\n","X = advert[feature_cols]\n","Y = advert['Sales']\n","trainX,testX,trainY,testY = train_test_split(X,Y, test_size = 0.2)\n","\n","lm = LinearRegression()\n","lm.fit(trainX, trainY)"],"execution_count":null,"outputs":[{"output_type":"stream","text":"/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n","name":"stderr"},{"output_type":"execute_result","execution_count":29,"data":{"text/plain":"LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"},"metadata":{}}]},{"metadata":{"id":"jrJ_SuIah8K4"},"cell_type":"markdown","source":["We split the advert dataset into train and test dataset and built the model on TV and radio variables from the test dataset. The following are the parameters of the model:"]},{"metadata":{"trusted":true,"id":"JjBuj5prh8K4","outputId":"1c570eca-f401-413c-867d-614ae3fac47c"},"cell_type":"code","source":["print (lm.intercept_ )\n","print (lm.coef_ )\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":"2.827652394729819\n[0.04501418 0.19464318]\nSolving environment: | ^C\nfailed\n\nCondaError: KeyboardInterrupt\n\n","name":"stdout"}]},{"metadata":{"id":"lhI-Uiizh8K4"},"cell_type":"markdown","source":["The result is as follows:\n","\n","(Note: your resulst may be slightly different depending on how the data has been sampled)\n","$$\n","Intercept – 2.8, TV coefficient – 0.04, Radio coefficient – 0.19\n","$$\n","A better way to look at the coefficients is to use the zip method to write the variable name and coefficient together. The required snippet and the output are mentioned in the following code:"]},{"metadata":{"trusted":true,"id":"0_s6MNwyh8K4","outputId":"7ae9acd7-1e47-4c43-d3df-3ef05c268887"},"cell_type":"code","source":["zip(feature_cols, lm.coef_)\n","[('TV', 0.045706061219705982), ('Radio', 0.18667738715568111)]"],"execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":34,"data":{"text/plain":"[('TV', 0.04570606121970598), ('Radio', 0.1866773871556811)]"},"metadata":{}}]},{"metadata":{"id":"hmlQutc5h8K5"},"cell_type":"markdown","source":["The value of $R^2$ is calculated by typing the following code:"]},{"metadata":{"trusted":true,"id":"uB54h-rgh8K5"},"cell_type":"code","source":["lm.score(trainX, trainY)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"dWpqvno6h8K5"},"cell_type":"markdown","source":["The value comes out to be around 0.89, very close to the value obtained by the method used earlier.\n","\n","The model can be used to predict the value of sales using TV and radio variables from the test dataset, as follows:"]},{"metadata":{"trusted":true,"id":"NSU06pcbh8K5","outputId":"ccde6971-5cfb-4426-e4ba-116b807b7dd2"},"cell_type":"code","source":["lm.predict(testX)"],"execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":35,"data":{"text/plain":"array([19.44538199,  3.49519546, 12.69573311, 19.24231623,  7.62225826,\n       20.8339672 , 11.46370881,  9.44062744, 21.43543249, 12.48026043,\n       15.5214624 , 11.61492194,  9.93281899, 17.68606434, 18.26591607,\n       20.80783707, 17.16363371, 12.60554723, 10.55360711, 14.94937555,\n       11.29654493, 14.40459359, 12.22843284,  9.54805007, 19.14778645,\n       13.14872992, 23.23540353, 13.65359363, 19.3020112 , 10.96456933,\n       23.36730898,  6.46691558, 10.56703217, 15.10582698,  6.4224505 ,\n       19.56017617, 14.96322423, 12.6053443 ,  9.76195376,  8.95983806])"},"metadata":{}}]},{"metadata":{"id":"O_aU69vAh8K5"},"cell_type":"markdown","source":["## Feature selection with scikit-learn\n","As stated before, many of the statistical tools and packages have in-built methods to conduct a variable selection process (forward selection and backward selection). If it is done manually, it will consume a lot of time and selecting the most important variables will be a tedious task compromising the efficiency of the model.\n","\n","One advantage of using the scikit-learn package for regression in Python is that it has this particular method for feature selection. This works more or less like backward selection (not exactly) and is called Recursive Feature Elimination (RFE). One can specify the number of variables they want in the final model.\n","\n","The model is first run with all the variables and certain weights are assigned to all the variables. In the subsequent iterations, the variables with the smallest weights are pruned from the list of variables till the desired number of variables is left.\n","\n","Let us see how one can do a feature selection in scikit-learn:"]},{"metadata":{"trusted":true,"id":"dWa0RNQjh8K5"},"cell_type":"code","source":["from sklearn.feature_selection import RFE\n","from sklearn.svm import SVR\n","feature_cols = ['TV', 'Radio','Newspaper']\n","X = advert[feature_cols]\n","Y = advert['Sales']\n","estimator = SVR(kernel=\"linear\")\n","selector = RFE(estimator,2,step=1)\n","selector = selector.fit(X, Y)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"oUPW852Rh8K5"},"cell_type":"markdown","source":["We use the methods named RFE and SVR in-built in scikit-learn. We indicate that we want to estimate a linear model and the number of desired variables in the model to be two.\n","To get the list of selected variables, one can write the following code snippet:"]},{"metadata":{"trusted":true,"id":"KtlSlsRHh8K5","outputId":"d5b88fff-20f9-4095-eb31-ce4991e7b46e"},"cell_type":"code","source":["selector.support_"],"execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":39,"data":{"text/plain":"array([ True,  True, False])"},"metadata":{}}]},{"metadata":{"id":"HvQxDi3th8K6"},"cell_type":"markdown","source":["It results in an array mentioning whether the variables in $X$ have been selected for the model or not. **True** means that the variable has been selected, while **False** means otherwise. In this case, the result is as follows:"]},{"metadata":{"id":"0sirPmTph8K6"},"cell_type":"markdown","source":["### Result of feature selection process\n","In our case, $X$ consists of three variables: TV, radio, and newspaper.\n","\n","The preceding array suggests that TV and radio have been selected for the model, while the newspaper hasn't been selected. This concurs with the variable selection we had done manually.\n","\n","This method also returns a ranking, as described in the following example:\n"]},{"metadata":{"trusted":true,"id":"3GGx3wyah8K6","outputId":"ad634df5-3f6b-452b-94e1-a9644e499dbc"},"cell_type":"code","source":["selector.ranking_"],"execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":40,"data":{"text/plain":"array([1, 1, 2])"},"metadata":{}}]},{"metadata":{"id":"UCVga6wYh8K6"},"cell_type":"markdown","source":["All the selected variables will have a ranking of 1 while the subsequent ones will be ranked in descending order of their significance. A variable with rank 2 will be more significant to the model than the one with a rank of 3 and so on.\n"]},{"metadata":{"trusted":true,"id":"vtXpiYzjh8K6"},"cell_type":"code","source":[],"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python36","display_name":"Python 3.6","language":"python"},"language_info":{"mimetype":"text/x-python","nbconvert_exporter":"python","name":"python","pygments_lexer":"ipython3","version":"3.6.6","file_extension":".py","codemirror_mode":{"version":3,"name":"ipython"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}